{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.datasets import imdb, cifar10, mnist\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Conv3D\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(\"Packages Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.random((1000, 500))\n",
    "y_train = np.random.randint(4, size=(1000, 1))\n",
    "one_hot_labels = tf.keras.utils.to_categorical(y_train, num_classes=4)\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(1000, input_dim=500, activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Dense(500,input_dim=1000,activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(150,input_dim=500,activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(units=4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, one_hot_labels, epochs=40, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.random((1000, 500))\n",
    "y_train = np.random.randint(4, size=(1000, 1))\n",
    "one_hot_labels = tf.keras.utils.to_categorical(y_train, num_classes=4)\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(1000, input_dim=500, activation='relu',kernel_initializer='uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(500,input_dim=1000,activation='sigmoid'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(150,input_dim=500,activation='sigmoid'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(units=4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, one_hot_labels, epochs=40, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.get_printoptions()['linewidth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[52], cmap = plt.cm.binary)\n",
    "plt.show()\n",
    "print(x_train[52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[52], cmap = plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(2500, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1500, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance = pd.read_csv('https://query.data.world/s/ihftnovt7fxbfo5zqro35sziekjz2w')\n",
    "target = insurance['charges']\n",
    "insurance = insurance.drop('charges', axis=1)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "insurance = insurance.apply(LabelEncoder().fit_transform)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(insurance, target, test_size=0.30, shuffle=False)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(2048, kernel_initializer='normal',input_dim = 6, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model.add(Dense(2048,activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(2048,activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(2048,activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model.add(Dense(1,activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.random((100, 100, 100, 3))\n",
    "y_train = tf.keras.utils.to_categorical(np.random.randint(4, size=(100, 1)), num_classes=4)\n",
    "x_test = np.random.random((20, 100, 100, 3))\n",
    "y_test = tf.keras.utils.to_categorical(np.random.randint(4, size=(20, 1)), num_classes=4)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile('Nadam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32, 32, 3), name='img')\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(1, 1), padding='same')(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(1, 1), padding='same')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "           \n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile('Nadam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "data_augmentation = True\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        zca_epsilon=1e-06,\n",
    "        rotation_range=0,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0., \n",
    "        zoom_range=0., \n",
    "        channel_shift_range=0.,\n",
    "        fill_mode='nearest',\n",
    "        cval=0., \n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False, \n",
    "        rescale=None,\n",
    "        preprocessing_function=None,\n",
    "        data_format=None,\n",
    "        validation_split=0.0)\n",
    "\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test))\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = 16\n",
    "timesteps = 16\n",
    "num_classes = 4\n",
    "batch_size = 64\n",
    "\n",
    "x_train = np.random.random((batch_size * 10, timesteps, data_dim))\n",
    "y_train = np.random.random((batch_size * 10, num_classes))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, stateful=True,\n",
    "               batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "model.add(LSTM(64, return_sequences=True, stateful=True))\n",
    "model.add(LSTM(64, stateful=True))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile('Nadam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100\n",
    "batch_size = 64\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('Nadam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100\n",
    "batch_size = 64\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('Nadam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 \n",
    "epochs = 50 \n",
    "latent_dim = 256 \n",
    "num_samples = 10000 \n",
    "\n",
    "data_path = 'fra.txt'\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, drop_text = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "    \n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(50):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),input_shape=(None, 40, 40, 1),padding='same', return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),padding='same', return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),padding='same', return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),padding='same', return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv3D(filters=1, kernel_size=(3, 3, 3),activation='sigmoid',padding='same', data_format='channels_last'))\n",
    "\n",
    "model.compile('Nadam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "def generate_movies(n_samples=1200, n_frames=15):\n",
    "    row = 80\n",
    "    col = 80\n",
    "    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n",
    "    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1),\n",
    "                              dtype=np.float)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        n = np.random.randint(3, 8)\n",
    "\n",
    "        for j in range(n):\n",
    "            xstart = np.random.randint(20, 60)\n",
    "            ystart = np.random.randint(20, 60)\n",
    "\n",
    "            directionx = np.random.randint(0, 3) - 1\n",
    "            directiony = np.random.randint(0, 3) - 1\n",
    "\n",
    "            w = np.random.randint(2, 4)\n",
    "\n",
    "            for t in range(n_frames):\n",
    "                x_shift = xstart + directionx * t\n",
    "                y_shift = ystart + directiony * t\n",
    "                noisy_movies[i, t, x_shift - w: x_shift + w,\n",
    "                             y_shift - w: y_shift + w, 0] += 1\n",
    "\n",
    "                if np.random.randint(0, 2):\n",
    "                    noise_f = (-1)**np.random.randint(0, 2)\n",
    "                    noisy_movies[i, t,\n",
    "                                 x_shift - w - 1: x_shift + w + 1,\n",
    "                                 y_shift - w - 1: y_shift + w + 1,\n",
    "                                 0] += noise_f * 0.1\n",
    "\n",
    "                x_shift = xstart + directionx * (t + 1)\n",
    "                y_shift = ystart + directiony * (t + 1)\n",
    "                shifted_movies[i, t, x_shift - w: x_shift + w,\n",
    "                               y_shift - w: y_shift + w, 0] += 1\n",
    "\n",
    "    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n",
    "    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n",
    "    noisy_movies[noisy_movies >= 1] = 1\n",
    "    shifted_movies[shifted_movies >= 1] = 1\n",
    "    return noisy_movies, shifted_movies\n",
    "\n",
    "noisy_movies, shifted_movies = generate_movies(n_samples=1200)\n",
    "model.fit(noisy_movies[:1000], shifted_movies[:1000], batch_size=10,\n",
    "        epochs=10, validation_split=0.05)\n",
    "\n",
    "which = 1004\n",
    "track = noisy_movies[which][:7, ::, ::, ::]\n",
    "\n",
    "for j in range(16):\n",
    "    new_pos = model.predict(track[np.newaxis, ::, ::, ::, ::])\n",
    "    new = new_pos[::, -1, ::, ::, ::]\n",
    "    track = np.concatenate((track, new), axis=0)\n",
    "\n",
    "track2 = noisy_movies[which][::, ::, ::, ::]\n",
    "for i in range(5):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "\n",
    "    if i >= 7:\n",
    "        ax.text(1, 3, 'Prediction', fontsize=20, color='w')\n",
    "    else:\n",
    "        ax.text(1, 3, 'Initial trajectory', fontsize=20)\n",
    "\n",
    "    toplot = track[i, ::, ::, 0]\n",
    "\n",
    "    plt.imshow(toplot)\n",
    "    ax = fig.add_subplot(122)\n",
    "    plt.text(1, 3, 'Ground truth', fontsize=20)\n",
    "\n",
    "    toplot = track2[i, ::, ::, 0]\n",
    "    if i >= 2:\n",
    "        toplot = shifted_movies[which][i - 1, ::, ::, 0]\n",
    "\n",
    "    plt.imshow(toplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(784,), name='img')\n",
    "x = Dense(128, activation='relu')(inputs)\n",
    "block_1_output = Dense(128, activation='relu')(x)\n",
    "\n",
    "x = Dense(128)(block_1_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "block_2_output = tf.keras.layers.add([x, block_1_output])\n",
    "\n",
    "x = Dense(128)(block_2_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "block_3_output = tf.keras.layers.add([x, block_2_output])\n",
    "\n",
    "x = Dense(128)(block_3_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "block_4_output = tf.keras.layers.add([x, block_3_output])\n",
    "\n",
    "x = Dense(128, activation='relu')(block_4_output)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name='resnet')\n",
    "\n",
    "model.compile(Adam(amsgrad=True), 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32, 32, 3), name='img')\n",
    "x = Conv2D(32, 3, activation='relu')(inputs)\n",
    "x = Conv2D(64, 3, activation='relu')(x)\n",
    "block_1_output = MaxPooling2D(3)(x)\n",
    "\n",
    "x = Conv2D(64, 3, padding='same')(block_1_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(64, 3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "block_2_output = tf.keras.layers.add([x, block_1_output])\n",
    "\n",
    "x = Conv2D(64, 3, padding='same')(block_2_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(64, 3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "block_3_output = tf.keras.layers.add([x, block_2_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu')(block_3_output)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name='resnet')\n",
    "\n",
    "model.compile(Adam(amsgrad=True), 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32, 32, 3), name='img')\n",
    "x = Conv2D(32, 3, activation='relu')(inputs)\n",
    "x = Conv2D(64, 3, activation='relu')(x)\n",
    "block_1_output = MaxPooling2D(3)(x)\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_1_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_2_output = tf.keras.layers.add([x, block_1_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_2_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_3_output = tf.keras.layers.add([x, block_2_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_3_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_4_output = tf.keras.layers.add([x, block_3_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_4_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_5_output = tf.keras.layers.add([x, block_4_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_5_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_6_output = tf.keras.layers.add([x, block_5_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_6_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_7_output = tf.keras.layers.add([x, block_6_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_7_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_8_output = tf.keras.layers.add([x, block_7_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_8_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_9_output = tf.keras.layers.add([x, block_8_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_9_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_10_output = tf.keras.layers.add([x, block_9_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_10_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_11_output = tf.keras.layers.add([x, block_10_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_11_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_12_output = tf.keras.layers.add([x, block_11_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_12_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_13_output = tf.keras.layers.add([x, block_12_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_13_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_14_output = tf.keras.layers.add([x, block_13_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_14_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_15_output = tf.keras.layers.add([x, block_14_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_15_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_16_output = tf.keras.layers.add([x, block_15_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_16_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_17_output = tf.keras.layers.add([x, block_16_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_17_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_18_output = tf.keras.layers.add([x, block_17_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_18_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_19_output = tf.keras.layers.add([x, block_18_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(block_19_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "block_20_output = tf.keras.layers.add([x, block_19_output])\n",
    "\n",
    "x = Conv2D(64, 3, activation='relu')(block_20_output)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name='resnet')\n",
    "\n",
    "\n",
    "model.compile(Adam(amsgrad=True), 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.random((1000, 500))\n",
    "y_train = np.random.randint(4, size=(1000, 1))\n",
    "one_hot_labels = tf.keras.utils.to_categorical(y_train, num_classes=4)\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(1000, input_dim=500, activation='relu',kernel_initializer='uniform'))\n",
    "model.add(Dense(500,input_dim=1000,activation='sigmoid'))\n",
    "model.add(Dense(150,input_dim=500,activation='sigmoid'))\n",
    "model.add(Dense(units=4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    model.fit(x_train, one_hot_labels, epochs=40, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
