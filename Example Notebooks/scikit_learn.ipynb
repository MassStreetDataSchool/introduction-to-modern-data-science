{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our standard packages for data science.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "# Our main packages for standard machine learning\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "print('Packages are ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winedata = pd.read_csv('Data\\winedata.csv')\n",
    "\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctarget = winedata['quality']\n",
    "qtarget = winedata['color']\n",
    "data = winedata.drop('quality', axis=1)\n",
    "data = data.drop('color', axis=1)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(winedata.shape)\n",
    "print(data.shape)\n",
    "print(qtarget.shape)\n",
    "print(ctarget.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = pd.DataFrame(boston.data)\n",
    "bos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos.columns = boston.feature_names\n",
    "bos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos['PRICE'] = boston.target\n",
    "bos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bos.RM, bos.PRICE)\n",
    "plt.xlabel(\"Average Rooms Per Dwelling (RM)\")\n",
    "plt.ylabel(\"Housing Price\")\n",
    "plt.title(\"Relationship between RM and Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(y=\"PRICE\", x=\"RM\", data=bos, fit_reg = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('PRICE ~ RM',bos).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('PRICE ~ CRIM',bos).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('PRICE ~ PTRATIO',bos).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('PRICE ~ RM+DIS+TAX+B+NOX+CRIM+ZN+INDUS+CHAS+AGE+RAD+PTRATIO+LSTAT',bos).fit()\n",
    "model.fittedvalues.head()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = ols('PRICE ~ PTRATIO+RM+CRIM+LSTAT',bos).fit()\n",
    "print(reduced.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bos.PRICE, model.fittedvalues)\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted prices\")\n",
    "plt.title(\"Prices vs Predicted Prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = bos.drop('PRICE', axis = 1)\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, bos.PRICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated intercept coefficient: {}'.format(lm.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'features': X.columns, 'estimatedCoefficients': lm.coef_})[['features', 'estimatedCoefficients']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lm.predict(X))\n",
    "plt.title('Predicted Housing Prices (fitted values)')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X[['PTRATIO']], bos.PRICE)\n",
    "msePTRATIO = np.mean((bos.PRICE - lm.predict(X[['PTRATIO']])) ** 2)\n",
    "print(msePTRATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X[['CRIM']], bos.PRICE)\n",
    "mseCRIM = np.mean((bos.PRICE - lm.predict(X[['CRIM']])) ** 2)\n",
    "print(mseCRIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X[['RM']], bos.PRICE)\n",
    "mseRM = np.mean((bos.PRICE - lm.predict(X[['RM']])) ** 2)\n",
    "print(mseRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X[['LSTAT']], bos.PRICE)\n",
    "mseLSTAT = np.mean((bos.PRICE - lm.predict(X[['LSTAT']])) ** 2)\n",
    "print(mseLSTAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data\n",
    "y = qtarget\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = LogisticRegression(n_jobs=-1)\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data\n",
    "y = qtarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = Lasso()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data\n",
    "y = qtarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data\n",
    "y = qtarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = LinearSVC()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data\n",
    "y = qtarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data\n",
    "y = qtarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Avg Accuracy RF', score / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdata = pd.read_csv('https://query.data.world/s/44aoyjg7hb4h64bns6iqnzpa5pmv2c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdata.isna().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ccdata['Time']\n",
    "\n",
    "plt.hist(x, bins=100)\n",
    "plt.ylabel('# of times')\n",
    "plt.xlabel('Dist of Time')\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ccdata[ccdata['Class']==1]['Time']\n",
    "\n",
    "plt.hist(x, bins=50)\n",
    "plt.ylabel('# of times')\n",
    "plt.xlabel('Dist of Time')\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdata['scaled_amount'] = RobustScaler().fit_transform(ccdata['Amount'].values.reshape(-1,1))\n",
    "ccdata['scaled_time'] = RobustScaler().fit_transform(ccdata['Time'].values.reshape(-1,1))\n",
    "\n",
    "ccdata.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "scaled_amount = ccdata['scaled_amount']\n",
    "scaled_time = ccdata['scaled_time']\n",
    "\n",
    "ccdata.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "ccdata.insert(0, 'scaled_amount', scaled_amount)\n",
    "ccdata.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "ccdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaled_amount\n",
    "del scaled_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When you run this line, you'll need to restart Jupyter Lab.\n",
    "!pip install --user imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import imblearn\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdata = ccdata.sample(frac=1, random_state=42)\n",
    "\n",
    "fraud_ccdata = ccdata[ccdata['Class'] == 1]\n",
    "non_fraud_ccdata = ccdata[ccdata['Class'] == 0][:492]\n",
    "\n",
    "undersample_ccdata = pd.concat([fraud_ccdata, non_fraud_ccdata])\n",
    "\n",
    "new_ccdata = undersample_ccdata.sample(frac=1, random_state=42)\n",
    "\n",
    "new_ccdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cctarget = new_ccdata['Class']\n",
    "cctarget = ccdata['Class']\n",
    "\n",
    "new_ccdata = new_ccdata.drop('Class', axis=1)\n",
    "ccdata = ccdata.drop('Class', axis=1)\n",
    "print('Data Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ccdata.reset_index(drop=True, inplace=True)\n",
    "new_cctarget.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data, training_target, test_target = train_test_split(ccdata, cctarget, test_size = .1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(training_data, training_target, test_size = .1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(sampling_strategy=.01, random_state=42)\n",
    "cc_smote, cc_smote_target = sm.fit_sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_smote_target.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(n_jobs=-1)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(n_jobs=-1)\n",
    "model.fit(new_ccdata, new_cctarget)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(n_jobs=-1)\n",
    "model.fit(cc_smote, cc_smote_target)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight= {0:.01, 1:.99},n_jobs=-1)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=0, n_jobs=-1)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_training_data, w_test_data, q_training_target, q_test_target = train_test_split(data, qtarget, test_size = .2, random_state=42)\n",
    "w_x_train, w_x_val, q_y_train, q_y_val = train_test_split(w_training_data, q_training_target, test_size = .1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to differences in machine resources, your times will differ from the screen and the lecture.\n",
    "#### Several scripts in this lecture will take a while to run. \n",
    "#### It will use a lot of resources and appear to make your machine crash.\n",
    "#### Just be patient, and check on it's progress periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Logistic Regression\n",
    "start = time.time()\n",
    "\n",
    "x = data\n",
    "y = qtarget\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = LogisticRegression(n_jobs=-1)\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print('Avg Accuracy RF', score / folds.n_splits)\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Lasso\n",
    "start = time.time()\n",
    "\n",
    "x = data\n",
    "y = qtarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = Lasso()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Avg Accuracy RF', score / folds.n_splits)\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Decision Tree\n",
    "start = time.time()\n",
    "\n",
    "x = data\n",
    "y = qtarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Avg Accuracy RF', score / folds.n_splits)\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Linear SVC\n",
    "start = time.time()\n",
    "\n",
    "x = data\n",
    "y = qtarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = LinearSVC()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Avg Accuracy RF', score / folds.n_splits)\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic K Nearest Neighbor\n",
    "start = time.time()\n",
    "\n",
    "x = data\n",
    "y = qtarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Avg Accuracy RF', score / folds.n_splits)\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Random Forest\n",
    "start = time.time()\n",
    "\n",
    "x = data\n",
    "y = qtarget\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=59)\n",
    "measured= np.zeros((x.shape[0]))\n",
    "score = 0\n",
    "\n",
    "for times, (trn_idx, val_idx) in enumerate(folds.split(x.values,y.values)):\n",
    "    model = RandomForestClassifier(n_jobs=-1)\n",
    "    model.fit(x.iloc[trn_idx],y[trn_idx])\n",
    "    measured[val_idx] = model.predict(x.iloc[val_idx])\n",
    "    score += model.score(x.iloc[val_idx],y[val_idx])\n",
    "    print(\"Fold: {} score: {}\".format(times,model.score(x.iloc[val_idx],y[val_idx])))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "print('Avg Accuracy RF', score / folds.n_splits)\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Grid 1\n",
    "\n",
    "| **Model**                 | **Fold 0 Score** | **Fold 1 Score** | **Fold 2 Score** | **Fold 3 Score** | **Fold 4 Score** | **Avg Accuracy RF** | **Run Time (s)** |\n",
    "|---------------------------|------------------|------------------|------------------|------------------|------------------|---------------------|------------------|\n",
    "| Basic Logistic Regression | 0.9838           | 0.9808           | 0.9838           | 0.9777           | 0.9838           | 0.9820              | 9.1450           |\n",
    "| Basic Lasso               | 0.4431           | 0.5150           | 0.4881           | 0.4896           | 0.5014           | 0.4874              | 0.4470           |\n",
    "| Basic Decision Tree       | 0.9854           | 0.9885           | 0.9838           | 0.9900           | 0.9861           | 0.9868              | 0.5620           |\n",
    "| Basic Linear SVC          | 0.6462           | 0.9823           | 0.9808           | 0.9861           | 0.9838           | 0.9158              | 1.2850           |\n",
    "| Basic K Nearest Neighbor  | 0.9385           | 0.9485           | 0.9453           | 0.9346           | 0.9453           | 0.9424              | 0.9460           |\n",
    "| Basic Random Forest       | 0.9900           | 0.9962           | 0.9962           | 0.9954           | 0.9946           | 0.9945              | 3.1260           |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sci Kit Learn's Grid Search for Decision Tree classifier\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [80, 120, None],\n",
    "    'max_features': ['auto', None],\n",
    "    'min_samples_leaf': [3, 25, 50],\n",
    "    'min_samples_split': [8, 10, 12]\n",
    "}\n",
    "model = GridSearchCV(DecisionTreeClassifier(), parameters, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "model.fit(data, qtarget)\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))\n",
    "model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sci Kit Learn's Grid Search for Logistic Regression\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "parameters = {\n",
    "    'tol': [1e-4, 1e-5, 1e-3],\n",
    "    'C': [.001, .01, .1, 1, 10, 100],\n",
    "    'max_iter': [100, 200, 50]\n",
    "}\n",
    "model = GridSearchCV(LogisticRegression(), parameters, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "model.fit(data, qtarget)\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))\n",
    "model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sci Kit Learn's Grid Search for Random Forest\n",
    "#This script could take an hour or more to exectute\n",
    "start = time.time()\n",
    "\n",
    "parameters = {\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [120, None],\n",
    "    'max_features': ['auto', None],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'n_estimators': [100, 500, 1000]\n",
    "}\n",
    "model = GridSearchCV(RandomForestClassifier(), parameters, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "model.fit(data, qtarget)\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))\n",
    "model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom grid search for Random Forest\n",
    "#This script takes a long time to run.\n",
    "start = time.time()\n",
    "\n",
    "grid = {\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [120, None],\n",
    "    'max_features': ['auto', None],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "best_score = 0\n",
    "\n",
    "for g in ParameterGrid(grid):\n",
    "    rf.set_params(**g)\n",
    "    rf.fit(w_x_train, q_y_train)\n",
    "    if rf.score(w_test_data, q_test_target) > best_score:\n",
    "        best_score = rf.score(w_test_data, q_test_target)\n",
    "        best_grid = g\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom grid search for Decision Trees\n",
    "start = time.time()\n",
    "\n",
    "grid = {\n",
    "    'max_depth': [80, 120, None],\n",
    "    'max_features': ['auto', None],\n",
    "    'min_samples_leaf': [3, 25, 50],\n",
    "    'min_samples_split': [8, 10, 12]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "best_score = 0\n",
    "\n",
    "for g in ParameterGrid(grid):\n",
    "    dt.set_params(**g)\n",
    "    dt.fit(w_x_train, q_y_train)\n",
    "    if dt.score(w_test_data, q_test_target) > best_score:\n",
    "        best_score = dt.score(w_test_data, q_test_target)\n",
    "        best_grid = g\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom grid search for Logistic Regression\n",
    "start = time.time()\n",
    "\n",
    "grid = {\n",
    "    'tol': [1e-4, 1e-5, 1e-3],\n",
    "    'C': [.001, .01, .1, 1, 10, 100],\n",
    "    'max_iter': [100, 200, 50],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "lg = LogisticRegression()\n",
    "best_score = 0\n",
    "\n",
    "for g in ParameterGrid(grid):\n",
    "    lg.set_params(**g)\n",
    "    lg.fit(w_x_train, q_y_train)\n",
    "    if lg.score(w_test_data, q_test_target) > best_score:\n",
    "        best_score = lg.score(w_test_data, q_test_target)\n",
    "        best_grid = g\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our baseline random forest classifier\n",
    "start = time.time()\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "model.fit(w_x_train, q_y_train)\n",
    "y_pred = model.predict(w_test_data)\n",
    "v_pred = model.predict(w_x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(w_x_val, q_y_val)))\n",
    "print('Recall: {}'.format(recall_score(q_y_val, model.predict(w_x_val))))\n",
    "print('Precision: {}'.format(precision_score(q_y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(q_y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(w_test_data, q_test_target)))\n",
    "print('Recall: {}'.format(recall_score(q_test_target, model.predict(w_test_data))))\n",
    "print('Precision: {}'.format(precision_score(q_test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(q_test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our custom grid search random forest classifier.\n",
    "start = time.time()\n",
    "\n",
    "# These parameters came from the grid search's output.\n",
    "model = RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='entropy', max_depth=120, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=8,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
    "                       n_jobs=-1, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "model.fit(w_x_train, q_y_train)\n",
    "y_pred = model.predict(w_test_data)\n",
    "v_pred = model.predict(w_x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(w_x_val, q_y_val)))\n",
    "print('Recall: {}'.format(recall_score(q_y_val, model.predict(w_x_val))))\n",
    "print('Precision: {}'.format(precision_score(q_y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(q_y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(w_test_data, q_test_target)))\n",
    "print('Recall: {}'.format(recall_score(q_test_target, model.predict(w_test_data))))\n",
    "print('Precision: {}'.format(precision_score(q_test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(q_test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SKLearn's grid search random forest classifier.\n",
    "start = time.time()\n",
    "\n",
    "# These parameters came from the grid search's output.\n",
    "model = RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='entropy', max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "                       n_jobs=-1, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "model.fit(w_x_train, q_y_train)\n",
    "y_pred = model.predict(w_test_data)\n",
    "v_pred = model.predict(w_x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(w_x_val, q_y_val)))\n",
    "print('Recall: {}'.format(recall_score(q_y_val, model.predict(w_x_val))))\n",
    "print('Precision: {}'.format(precision_score(q_y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(q_y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(w_test_data, q_test_target)))\n",
    "print('Recall: {}'.format(recall_score(q_test_target, model.predict(w_test_data))))\n",
    "print('Precision: {}'.format(precision_score(q_test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(q_test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline decision tree classifier.\n",
    "start = time.time()\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(w_x_train, q_y_train)\n",
    "y_pred = model.predict(w_test_data)\n",
    "v_pred = model.predict(w_x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(w_x_val, q_y_val)))\n",
    "print('Recall: {}'.format(recall_score(q_y_val, model.predict(w_x_val))))\n",
    "print('Precision: {}'.format(precision_score(q_y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(q_y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(w_test_data, q_test_target)))\n",
    "print('Recall: {}'.format(recall_score(q_test_target, model.predict(w_test_data))))\n",
    "print('Precision: {}'.format(precision_score(q_test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(q_test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom decision tree classifier.\n",
    "start = time.time()\n",
    "\n",
    "# These parameters came from the grid search's output.\n",
    "model = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=3, min_samples_split=8,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=None, splitter='best')\n",
    "model.fit(w_x_train, q_y_train)\n",
    "y_pred = model.predict(w_test_data)\n",
    "v_pred = model.predict(w_x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(w_x_val, q_y_val)))\n",
    "print('Recall: {}'.format(recall_score(q_y_val, model.predict(w_x_val))))\n",
    "print('Precision: {}'.format(precision_score(q_y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(q_y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(w_test_data, q_test_target)))\n",
    "print('Recall: {}'.format(recall_score(q_test_target, model.predict(w_test_data))))\n",
    "print('Precision: {}'.format(precision_score(q_test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(q_test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom logistic  regression\n",
    "start = time.time()\n",
    "\n",
    "# These parameters came from the grid search's output.\n",
    "model = LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
    "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "model.fit(w_x_train, q_y_train)\n",
    "y_pred = model.predict(w_test_data)\n",
    "v_pred = model.predict(w_x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(w_x_val, q_y_val)))\n",
    "print('Recall: {}'.format(recall_score(q_y_val, model.predict(w_x_val))))\n",
    "print('Precision: {}'.format(precision_score(q_y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(q_y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(w_test_data, q_test_target)))\n",
    "print('Recall: {}'.format(recall_score(q_test_target, model.predict(w_test_data))))\n",
    "print('Precision: {}'.format(precision_score(q_test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(q_test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(q_test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Grid 2\n",
    "\n",
    "| **Model**                                                 | **Accuracy** | **Recall** | **Precision** | **f1** | **TP** | **FP** | **FN** | **TN** | **Run Time** |\n",
    "|-----------------------------------------------------------|--------------|------------|---------------|--------|--------|--------|--------|--------|--------------|\n",
    "| baseline random forest classifier validation              | 0.9981       | 1.0000     | 0.9981        | 0.9981 | 120    | 1      | 0      | 399    | 0.9370       |\n",
    "| baseline random forest classifier test                    | 0.9938       | 0.9990     | 0.9938        | 0.9938 | 334    | 7      | 1      | 958    | 0.9370       |\n",
    "| custom grid search random forest classifier validation    | 0.9981       | 1.0000     | 0.9981        | 0.9981 | 120    | 1      | 0      | 399    | 3.7790       |\n",
    "| custom grid search random forest classifier test          | 0.9962       | 0.9990     | 0.9962        | 0.9962 | 337    | 4      | 1      | 958    | 3.7790       |\n",
    "| SKLearn's grid search random forest classifier validation | 0.9981       | 1.0000     | 0.9981        | 0.9981 | 120    | 1      | 0      | 399    | 2.0210       |\n",
    "| SKLearn's grid search random forest classifier test       | 0.9962       | 0.9990     | 0.9962        | 0.9962 | 337    | 4      | 1      | 958    | 2.0210       |\n",
    "| baseline decision tree classifier validation              | 0.9904       | 0.9950     | 0.9904        | 0.9904 | 118    | 3      | 2      | 397    | 0.0620       |\n",
    "| baseline decision tree classifier test                    | 0.9831       | 0.9906     | 0.9831        | 0.9831 | 328    | 13     | 9      | 950    | 0.0620       |\n",
    "| Custom decision tree classifier validation                | 0.9904       | 0.9950     | 0.9904        | 0.9904 | 118    | 3      | 2      | 397    | 0.0510       |\n",
    "| Custom decision tree classifier test                      | 0.9815       | 0.9927     | 0.9815        | 0.9815 | 324    | 17     | 7      | 952    | 0.0510       |\n",
    "| Custom logistic regression validation                     | 0.9865       | 0.9875     | 0.9865        | 0.9865 | 119    | 2      | 5      | 394    | 0.2560       |\n",
    "| Custom logistic regression test                           | 0.9808       | 0.9875     | 0.9808        | 0.9808 | 328    | 13     | 12     | 947    | 0.2560       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following models are run with the credit card data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Credit Card Data Decision Tree Classifier.\n",
    "start = time.time()\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier with classes weighted .2:.8\n",
    "start = time.time()\n",
    "\n",
    "model = DecisionTreeClassifier(class_weight= {0:.2, 1:.8})\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with Smote data\n",
    "#this is a long running script.\n",
    "start = time.time()\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(cc_smote, cc_smote_target)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with classes weighted .2:.8 and smote data\n",
    "# this is a long running script.\n",
    "start = time.time()\n",
    "\n",
    "model = RandomForestClassifier(class_weight= {0:.2, 1:.8})\n",
    "model.fit(cc_smote, cc_smote_target)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with smote data and 1,000 estimators and bootstrap False\n",
    "# This is a long running script.\n",
    "start = time.time()\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=1000, bootstrap=False, n_jobs=-1)\n",
    "model.fit(cc_smote, cc_smote_target)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with the random undersampled data\n",
    "start = time.time()\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(new_ccdata, new_cctarget)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Grid search to find the best random forest model with the undersampled data.\n",
    "#This is a long running script.\n",
    "start = time.time()\n",
    "\n",
    "grid = {\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [120, None],\n",
    "    'max_features': ['auto', None],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "best_score = 0\n",
    "\n",
    "for g in ParameterGrid(grid):\n",
    "    rf.set_params(**g)\n",
    "    rf.fit(new_ccdata, new_cctarget)\n",
    "    # save if best\n",
    "    if rf.score(test_data, test_target) > best_score:\n",
    "        best_score = rf.score(test_data, test_target)\n",
    "        best_grid = g\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our best custom random forest model for the undersampled dataset. Also our best model for this data! \n",
    "start = time.time()\n",
    "\n",
    "model = RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='entropy', max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=6, min_samples_split=4,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
    "                       n_jobs=-1, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "model.fit(new_ccdata, new_cctarget)\n",
    "y_pred = model.predict(test_data)\n",
    "v_pred = model.predict(x_val)\n",
    "\n",
    "print('Validation Results')\n",
    "print('Accuracy: {}'.format(model.score(x_val, y_val)))\n",
    "print('Recall: {}'.format(recall_score(y_val, model.predict(x_val))))\n",
    "print('Precision: {}'.format(precision_score(y_val, v_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(y_val, v_pred, average='micro')))\n",
    "print(confusion_matrix(y_val, v_pred))\n",
    "\n",
    "print('\\nTest Results')\n",
    "print('Accuracy: {}'.format(model.score(test_data, test_target)))\n",
    "print('Recall: {}'.format(recall_score(test_target, model.predict(test_data))))\n",
    "print('Precision: {}'.format(precision_score(test_target, y_pred, average='micro')))\n",
    "print('f1: {}'.format(f1_score(test_target, y_pred, average='micro')))\n",
    "print(confusion_matrix(test_target, y_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('It took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Grid 3\n",
    "\n",
    "| **Model**                                                                                    | **Accuracy** | **Recall** | **Precision** | **f1** | **TP** | **FP** | **FN** | **TN** | **Run Time** |\n",
    "|----------------------------------------------------------------------------------------------|--------------|------------|---------------|--------|--------|--------|--------|--------|--------------|\n",
    "| Baseline Credit Card Data Decision Tree Classifier validation                                | 0.9991       | 0.7963     | 0.9991        | 0.9991 | 25567  | 12     | 11     | 43     | 23.012       |\n",
    "| Baseline Credit Card Data Decision Tree Classifier test                                      | 0.9992       | 0.8148     | 0.9992        | 0.9992 | 28413  | 14     | 10     | 44     | 23.012       |\n",
    "| Decision Tree Classifier with classes weighted .2:.8 validation                              | 0.9991       | 0.8148     | 0.9991        | 0.9991 | 25567  | 12     | 10     | 44     | 20.489       |\n",
    "| Decision Tree Classifier with classes weighted .2:.8 test                                    | 0.9994       | 0.8333     | 0.9994        | 0.9994 | 28420  | 7      | 9      | 45     | 20.489       |\n",
    "| Random Forest Classifier with Smote data validation                                          | 0.9995       | 0.8333     | 0.9995        | 0.9995 | 25574  | 5      | 9      | 45     | 228.916      |\n",
    "| Random Forest Classifier with Smote data test                                                | 0.9997       | 0.9074     | 0.9997        | 0.9997 | 28423  | 4      | 5      | 49     | 228.916      |\n",
    "| Random Forest Classifier with classes weighted .2:.8 and smote data validation               | 0.9995       | 0.8333     | 0.9995        | 0.9995 | 25575  | 4      | 9      | 45     | 203.673      |\n",
    "| Random Forest Classifier with classes weighted .2:.8 and smote data test                     | 0.9997       | 0.8889     | 0.9997        | 0.9997 | 28424  | 3      | 6      | 48     | 203.673      |\n",
    "| Random Forest Classifier with smote data and 1,000 estimators and bootstrap False validation | 0.9995       | 0.8333     | 0.9995        | 0.9995 | 25575  | 4      | 9      | 45     | 952.100      |\n",
    "| Random Forest Classifier with smote data and 1,000 estimators and bootstrap False test       | 0.9997       | 0.9074     | 0.9997        | 0.9997 | 28423  | 4      | 5      | 49     | 952.100      |\n",
    "| Random Forest Classifier with the random undersampled data validation                        | 0.9774       | 1.0000     | 0.9774        | 0.9774 | 25000  | 579    | 0      | 54     | 2.097        |\n",
    "| Random Forest Classifier with the random undersampled data test                              | 0.9759       | 1.0000     | 0.9759        | 0.9759 | 27742  | 685    | 0      | 54     | 2.097        |\n",
    "| Our best custom random forest model for the undersampled dataset validation                  | 0.9827       | 1.0000     | 0.9827        | 0.9827 | 25136  | 443    | 0      | 54     | 6.739        |\n",
    "| Our best custom random forest model for the undersampled dataset test                        | 0.9824       | 0.9815     | 0.9824        | 0.9824 | 27926  | 501    | 1      | 53     | 6.739        |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
